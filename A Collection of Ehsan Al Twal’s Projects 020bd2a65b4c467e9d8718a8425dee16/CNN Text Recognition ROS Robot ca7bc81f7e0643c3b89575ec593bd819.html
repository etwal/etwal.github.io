<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>CNN Text Recognition ROS Robot</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="ca7bc81f-7e06-43c3-b895-75ec593bd819" class="page sans"><header><h1 class="page-title">CNN Text Recognition ROS Robot</h1><p class="page-description"></p></header><div class="page-body"><h2 id="136bdb76-a201-4e93-8fde-78d7eb7842a3" class="block-color-default">←<a href="https://etwal.github.io/"><strong>Home Page</strong></a></h2><figure id="c9add32d-8d9d-4a8c-9116-04faa75a4008" class="image"><a href="trim.72FF6F55-20B2-4DAA-9478-36F7B9D181CB-ezgif.com-optimize.gif"><img style="width:600px" src="trim.72FF6F55-20B2-4DAA-9478-36F7B9D181CB-ezgif.com-optimize.gif"/></a><figcaption>Recording of the robot doing a round, on the top left we see the robot driving around the course, on the top right we see the clues being submitted under the column “Predicted” and we see that the NN is guessing the clues correctly since we awarded the points. In the center, we have a window of the images of the clues. The bottom left is the terminal used to start the program</figcaption></figure><p id="ae2c576f-e6ef-4a76-8be8-0b202c232fc3" class="">The video above may take a while to load sorry for the inconvenience. It is a sped up version of one of the successful runs that the robot has made</p><p id="83b3a650-ec40-4d4b-8c7f-6112f7fb5ace" class="">For this project, I programmed the robot to do the following:</p><ul id="7ebd36d4-899d-4b55-a4d0-b030712dd824" class="bulleted-list"><li style="list-style-type:disc">Drive on the road using PID and detect a pedestrian and a car to avoid colliding with them</li></ul><ul id="fe8cdbbf-d8c5-4148-926c-20b0dbce590c" class="bulleted-list"><li style="list-style-type:disc">Detect the clues on the side of the road and take clear images of them</li></ul><ul id="4859021e-7a85-4b86-97c5-7e634209d569" class="bulleted-list"><li style="list-style-type:disc">A Convolution Neural Network that takes the images of the clues as input and returns the text that is in the image</li></ul><h2 id="3051a59d-3653-42c6-bef6-71662e092816" class="">Driving</h2><p id="73b5f90e-f47e-468b-a37d-9e1c8b6d3ab7" class="">For the driving, I used PID to navigate the course. To start with, I created a class for all the functions I created.</p><p id="069a9ddf-3610-4bb2-9700-a8bc6b402f94" class="">The class had an init function and in it, I initialized the subscriptions and publications, and I initialized global variables that were used later in the code</p><p id="360e1f02-b141-4551-8619-88adda376460" class="">The main function for driving was camera_callback. This function would be called every time the camera sends a new frame.</p><p id="70a0be58-e9d5-404c-b9e8-41ea9ad7a136" class="">The code is split into different segments based on where we are driving on the map</p><h3 id="35aabfec-1cbb-4a25-8c11-da3932a8bc37" class="">Part 1 is driving on the road</h3><p id="7750e89c-1833-4c16-91ea-81b8222aa15b" class="">For this part, I used an HSV mask to filter out everything in the frame other than the white lines on the side of the road</p><p id="9f5d2c5d-be37-484a-9026-72c487ef562f" class="">Then I would find the contours put them in a list and sort that list from largest to smallest contour</p><p id="d0c09f1e-ed88-4107-b3ee-995bf1784898" class="">After that, I would calculate the midpoint between the biggest two contours.</p><figure id="d5131a27-9900-436f-9619-820b34caa4cf" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled.png"><img style="width:2000px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled.png"/></a><figcaption>bottom right is the image from the camera on the robot, top left is the environment, and the window ‘ROAD’ shows the calculated midpoint on the frame</figcaption></figure><p id="55422128-ae35-4d1a-a305-0b3e713a57b1" class="">Then I would use the difference between the midpoint (red circle) and the center of the frame to determine the angular velocity the car should have. </p><p id="6ca84956-15a2-47d5-acdc-9eaf6e89f3ab" class="">The variable self.pos is the value of the angular velocity.</p><p id="53203b8e-d7f1-45f7-ac45-283eeb75fbbb" class="">This code is used when the crossroad is not in view. Therefore if the crossroad is not in view then I publish the velocity.</p><p id="a158e49e-befc-45a8-b08e-65af464b515b" class="">When I reached the crossroad. I use another HSV filter on the original frame to filter out everything but the red color and then I would look for the number of pixels of red, if they are greater than 5% of the pixels in frame then I change the variable no_crossroad to false.</p><figure id="b2cb1abb-f5c7-4a87-8d69-e0978a214f27" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%201.png"><img style="width:2000px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%201.png"/></a><figcaption>Red strip indicates that there is a crossroad ahead</figcaption></figure><p id="a75fb25e-e427-4208-a34f-ab8e481b6d61" class="">That triggers the handle crossroad function, in this function I stop the car by setting the velocities to zero</p><p id="7be19b28-6702-42a0-a8c2-d73c64b750d4" class="">Then I would use an HSV filter to filter everything but the pedestrian, I used the color of the pedestrian&#x27;s pants for this filter. I then would find the centroid of the pedestrian and if it is between 600 and 700 (this is about the center of the crossroad) then I would check if the pedestrian is going to the right by checking if the previous centroid is smaller than the current centroid.</p><figure id="69524421-c105-48b7-b087-cb3db09942dd" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%202.png"><img style="width:2000px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%202.png"/></a><figcaption>The bottom left is the image the camera on the robot sees/sends, and the bottom left is the image after applying the filter for the pants</figcaption></figure><p id="5bccca7e-ebaa-4478-96b5-cbfa2c47c84f" class="">Then the car drives normally until we reach the roundabout. Here the function car handler is triggered, inn that function I use an hsv filter to filter everything but the car, then I would monitor the number of pixels of the car until it is 9500, that is the point when the car passes by the entrance of the roundabout after that happens I have a delay and the robot makes a left turn into the roundabout. </p><figure id="42744053-1b7d-4d7c-9416-20c8cb430bde" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%203.png"><img style="width:2000px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%203.png"/></a><figcaption>Car filter frame </figcaption></figure><p id="5657bdef-f5ba-47be-949d-e4258ad8fd24" class="">Then I make the saw_car variable True so that this code does not trigger again</p><p id="8f9b6b82-452e-4622-86f1-93d3b4ce8906" class="">After that, my objective is for the car to exit the roundabout. When the number of contours in view is one and the contour is on the right that means we are at the exit because there is no white line on the left side at that point of the roundabout.</p><p id="59d4ac01-df79-4d8f-8c03-34a16d888309" class="">To make sure this piece of code does not trigger anywhere I used a variable to indicate that the robot is in the roundabout and this has to happen for 11 frames in a row. Once that happens the car makes a left turn and exits the roundabout. </p><p id="09f00439-cbc7-4af4-8e6f-8c347e62bcc8" class="">After that, I used an hsv filter to remove everything but the purple/pink line in the frame. Once the purple pixels in view are greater than 10% I would turn the variable on_grass to true </p><figure id="0e1328b6-80ae-4f4e-9df7-c1f6e778a78d" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%204.png"><img style="width:2000px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%204.png"/></a><figcaption>Purple line to indicate the begging of the grass section</figcaption></figure><h3 id="ce2e06ba-4705-4835-9bc9-9d916e3ffa91" class="">Part 2 is driving on the grass</h3><p id="54f951e5-800a-4ff8-942f-b3e989491500" class="">I first apply an hsv filter to remove almost everything but the lines on the side of the road. The problem here is that there are a lot of patches of grass that have the same colour as the lines on the side of the road. Therefore I wrote a code that would check if a contour is rectangular for the four largest contours in view</p><p id="110a4ad7-abca-4b98-9bc3-65ce2920d9f5" class="">Then I would find the y value of each rectangle and I would choose the two contours with the smallest y (this means they were closest to the top of the frame which is what the sidelines should have)</p><p id="976546a5-4f9d-41a0-8382-399ff1d9d817" class="">Then I would use the same code from the on-road section to find the midpoint between both lines and make the car follow that.</p><figure id="12dc833a-a91f-4f2b-ba22-1377a7fb9952" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%205.png"><img style="width:2000px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%205.png"/></a></figure><p id="3db3f003-d110-43e9-8dc0-d46be048d06c" class="">I referred to Contour Features in OpenCV (OpenCV, 2023) to learn about contours and their functions which I used in my code. I used the contour area function to sort the contours from largest to smallest which helped me find the contours of the sidelines as they were one of the largest if not the largest contours in the frame and I used the bounding rectangle function to find the y values of the contours which I later used to select the contours with the closest y values to the top of the frame as those were the contours of the sidelines.</p><p id="a9d2b683-8203-400d-a398-f636db1425ed" class="">Lastly, I would again look for the purple pixels in the frame and once that is more than 10% of the pixels in the frame the robot starts driving towards the line so we can teleport to the next pink line and get the 7th clue.</p><figure id="90eee3ce-f815-4af6-87bf-468fd3e6ffe9" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%206.png"><img style="width:2000px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%206.png"/></a></figure><h3 id="b6276f08-0084-4943-8221-783a0d3f5194" class="">Clue detection</h3><p id="5399acee-3ac3-4dca-8bb4-f975ca7817af" class="">The robot is always looking for the clue boards, theoretically, this slows down performance. However, the code was relatively simple and with a real-time factor of 0.8 everything integrated well.</p><p id="fa2aac26-4972-41e4-9e8a-80e312e8c96e" class="">To detect a clue board a simple blue mask is applied to the frame. The resulting frame would then be processed for contours and the largest one identified, which is then cropped according to the contour’s bounding rectangle. The cropped image is only taken into account if its dimensions are of a reasonable scale, if the board contour is a quadrilateral and the position of the board is such that the board is fully in view, i.e. is not half in the frame. This all occurs in the detect_clue method.</p><p id="afb52cdf-f3fe-4825-a376-5e381d767bba" class="">If a clue is detected then it is passed into the perspective_clue method where the image is inverted and a mask is used to find the corners of the white section of the board. These points are then sorted and used to apply a perspective transform using cv2’s warpPerspective method.</p><p id="98970db6-ca14-45bc-9200-08f7637c5fe2" class="">All this processing was vital to retrieve an image good enough to use to predict the letters using the neural network. These methods were planned and were very straightforward to implement with the help of Opencv2 documentation.</p><p id="613a662e-43a1-498e-a119-9e6081e74ed0" class="">A list is created with the images and is appended as long as there is a clue board in sight. As soon as it is lost from sight the message is sent to the score tracker.</p><h3 id="4814abf6-b025-4281-b86e-ce85fb539e62" class="">Data Engine Used</h3><p id="434101d6-9711-472d-8d59-7898aa19a8f4" class="">Procuring data was a long process filled with trial and error. The data used was one of two sets, one generated using cv2 and the other collected from the environment itself. The generated data was a set of single letters rotated, and blurred to different degrees.</p><p id="c0932a55-ba8f-4aab-8a1a-37f6024b3fff" class="">The rest of the data was collected from the simulation. To mitigate any bias that could arise from lighting or other uncontrolled variables, an image of each letter was selected from each individual board. These letters were then isolated into a  list of images using some code and then augmented similarly to the generated data. The network trained on this data gave accurate predictions and therefore was the one finally used.</p><h3 id="3cfd445f-33e0-45d3-bf68-36c3145bcacb" class="">Neural Network Architecture</h3><p id="db40d645-5ae8-4f89-abcc-e4aef2df8e0a" class="">The following is the network architecture used in this application:</p><figure id="5be4f7c3-ef5d-46f0-9989-2f041b1d6a0f" class="image"><a href="https://lh7-us.googleusercontent.com/qS02nTJBgr9j1W94pUCU9lC6qrAU3hZcEmZ4MhAX49ty4iK9BXjgCvRB18gqtGvYOSkK4LWrQIRQmm3ys3QTHfpbtdvHaFpKhKRnU0aQLZFNKFW4KheXpbwYmBffnPfRCoAMO2ZXd3C_DptllJdQEA"><img style="width:419px" src="https://lh7-us.googleusercontent.com/qS02nTJBgr9j1W94pUCU9lC6qrAU3hZcEmZ4MhAX49ty4iK9BXjgCvRB18gqtGvYOSkK4LWrQIRQmm3ys3QTHfpbtdvHaFpKhKRnU0aQLZFNKFW4KheXpbwYmBffnPfRCoAMO2ZXd3C_DptllJdQEA"/></a></figure><p id="9e975247-c36d-4b24-8f5c-4ea3621821dc" class="">The architecture used is identical to that used in the lab and mentioned in the lecture with input shape =(50, 50, 1). All activation functions were set to Relu except the last layer which was softmax.</p><h3 id="b38ab26d-4066-4bb0-a88a-0fae663477cc" class="">Training Parameters</h3><p id="96b52f08-9979-4936-afdb-91cd3b249bd1" class="">After getting reasonable quality training data not much change was done to the training parameters as the neural network trained performed well. The learning rate was dropped from 1e-4 as that created to much oscillations in the loss</p><p id="4723d210-aa47-4ce7-892b-592b6ebafcc3" class="">The following are the training parameters used:</p><ul id="4a04304f-addc-4113-bad2-89416b7936b7" class="bulleted-list"><li style="list-style-type:disc">Learning rate = 1e-3</li></ul><ul id="bb3bcaa2-6dc4-44bc-888f-c1ae768af0d3" class="bulleted-list"><li style="list-style-type:disc">Number of epochs= 200</li></ul><ul id="333a9166-ba4e-45fe-89eb-204e378dea5a" class="bulleted-list"><li style="list-style-type:disc">Dropout = 0.5</li></ul><ul id="d093f7d6-402b-4f7a-8453-8b0ddf4df494" class="bulleted-list"><li style="list-style-type:disc">Data set size and composition: 5328 characters of equal distribution, 123 generated per letter, the rest was augmented sim data.</li></ul><figure id="8900b717-6591-4d0f-9172-054120d37b70" class="image"><a href="https://lh7-us.googleusercontent.com/UTdw7O6PtC_CTm_1rWECGuedtqmBH-KzljpiJeyiXXglmhL0V0yjpCsitzFYDKGDFherSQZ_yCBLX5jAfTCWHAZaHglRiiEzvtoKHNz76Jx3kz300P03A7Yj8iLvtuwwLOFlQixoIo3dB-L-1DjK8Q"><img style="width:624px" src="https://lh7-us.googleusercontent.com/UTdw7O6PtC_CTm_1rWECGuedtqmBH-KzljpiJeyiXXglmhL0V0yjpCsitzFYDKGDFherSQZ_yCBLX5jAfTCWHAZaHglRiiEzvtoKHNz76Jx3kz300P03A7Yj8iLvtuwwLOFlQixoIo3dB-L-1DjK8Q"/></a></figure><h3 id="d36974a9-ec3a-485b-b16a-f1ec0da07455" class="">Training and Validation Test performance</h3><p id="b6bf88f1-8278-419d-8fb2-bea1adb82b79" class="">The model loss followed as expected, if not converging too fast. That was suspicious at the start but the trained model worked flawlessly upon integration. The validation data loss was low from the start but that is expected as they are sourced from the same place as the training data. The model accuracy follows similarly to the loss be it starting low. Again this was suspicious. However, upon further analysis, it can be seen that due to our batch size most, if not all, letters would be introduced to the network within the first couple of epochs. Therefore, it seems the network learned quickly and the variables started somewhere close to a minimum in the data space.</p><figure id="a1fd323d-e920-4105-a2df-d4b147ae81e2" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%207.png"><img style="width:716px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%207.png"/></a></figure><figure id="793acabe-33f0-4426-bb77-28d02651b1ac" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%208.png"><img style="width:770px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%208.png"/></a></figure><figure id="db0832bf-2759-4249-a2f5-2fb2126e6de8" class="image"><a href="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%209.png"><img style="width:888px" src="CNN%20Text%20Recognition%20ROS%20Robot%20ca7bc81f7e0643c3b89575ec593bd819/Untitled%209.png"/></a></figure></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>